# -*- coding: utf-8 -*-
"""PySpark Notebook de Pipeline de Seleção de Features para Regressão Logística com Classe Minoritária (com VIF).ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-YOUR_GOOGLE_DRIVE_FILE_ID
"""

# ### PySpark Notebook de Pipeline de Seleção de Features para Regressão Logística com Classe Minoritária (com VIF)

# This notebook demonstrates a complete pipeline for performing feature selection in a dataset with many features (900) and a minority class (4%), aiming to train a logistic regression model using PySpark, now including a VIF calculation.

# ### 1. Initialize Spark Session

from pyspark.sql import SparkSession
from pyspark.sql.functions import col, isnull, when, count, lit, mean, median
from pyspark.ml.feature import VectorAssembler, StandardScaler, MinMaxScaler
from pyspark.ml.stat import Correlation
from pyspark.ml.feature import ChiSqSelector, RFE
from pyspark.ml.classification import LogisticRegression
from pyspark.ml.evaluation import BinaryClassificationEvaluator
from pyspark.ml import Pipeline
from pyspark.sql.window import Window
from pyspark.sql.functions import percentile_approx
from pyspark.ml.tuning import ParamGridBuilder, CrossValidator
from imblearn.over_sampling import SMOTE  # For handling imbalance (will be applied after collection for Spark)
import pandas as pd
from pyspark.ml.regression import LinearRegression
from pyspark.sql.types import DoubleType
from pyspark.ml.evaluation import MulticlassClassificationEvaluator
import matplotlib.pyplot as plt
import seaborn as sns
from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, classification_report

# Initialize Spark Session
spark = SparkSession.builder.appName("FeatureSelectionLogisticRegressionVIF").getOrCreate()

# ### 2. Load and Explore Data

# **Replace 'your_file.csv' with the path to your data file.**
try:
    pandas_df = pd.read_csv('your_file.csv')
    df = spark.createDataFrame(pandas_df)
    print("Data loaded successfully!")
    print(f"Number of samples: {df.count()}")
    print(f"Number of features: {len(df.columns) - 1} (assuming the last column is the target)")
    print("\nSchema of the DataFrame:")
    df.printSchema()
    print("\nDistribution of the target class:")
    df.groupBy(df.columns[-1]).count().withColumn("percentage", col("count") / df.count()).show() # Assuming the last column is the target
except FileNotFoundError:
    print("Error: File 'your_file.csv' not found. Please verify the file path.")
    spark.stop()
    exit()

# Target column name (adjust if necessary)
target_column = df.columns[-1]
feature_columns = [col for col in df.columns if col != target_column]

# ### 3. Evaluate and Handle Missing Values

def evaluate_and_handle_missing_spark(df, feature_columns):
    """
    Evaluates and handles missing values in a Spark DataFrame.

    Args:
        df (pyspark.sql.DataFrame): The input DataFrame.
        feature_columns (list): List of feature column names.

    Returns:
        pyspark.sql.DataFrame: The DataFrame after handling missing values.
        list: List of columns removed due to a high percentage of missing values.
    """

    n_rows = df.count()

    # 1. Identify the percentage of missing values per column
    missing_counts = df.select([count(when(isnull(c), c)).alias(c) for c in feature_columns])
    missing_percentage = {col: missing_counts.first()[col] / n_rows * 100 for col in feature_columns}

    print("Percentage of Missing Values per Column:")
    for col, percentage in sorted(missing_percentage.items(), key=lambda item: item[1], reverse=True):
        if percentage > 0:
            print(f"{col}: {percentage:.2f}%")

    # 2. Decide how to handle (e.g., remove columns with many missing, impute others)
    # Example: Remove columns with more than 50% missing
    threshold_remover_colunas = 50
    colunas_para_remover = [col for col, percentage in missing_percentage.items() if percentage > threshold_remover_colunas]
    df = df.drop(*colunas_para_remover)
    print(f"\nColumns removed ({len(colunas_para_remover)}): {colunas_para_remover}")
    feature_columns = [col for col in feature_columns if col not in colunas_para_remover]

    # Example: Impute remaining missing values with the median (robust to outliers)
    for col_impute in feature_columns:
        median_val = df.approxQuantile(col_impute, [0.5], 0.01)[0] # Using approxQuantile for efficiency on large datasets
        df = df.fillna({col_impute: median_val})
        print(f"Missing values in column '{col_impute}' imputed with the median: {median_val}")

    # 3. Verify if there are still missing values in the remaining features
    remaining_missing = df.select([count(when(isnull(c), c)).alias(c) for c in feature_columns])
    remaining_missing_dict = remaining_missing.first().asDict()
    total_remaining_missing = sum(remaining_missing_dict.values())
    print(f"\nRemaining missing values in features: {total_remaining_missing}")

    return df, feature_columns, colunas_para_remover

df_tratado, feature_columns_tratados, colunas_removidas_missing = evaluate_and_handle_missing_spark(df, feature_columns)

# ### 4. Variance Inflation Factor (VIF) Calculation (Iterative)

def calculate_vif_spark(df, feature_columns):
    """
    Calculates the Variance Inflation Factor (VIF) for each feature in a Spark DataFrame iteratively.

    Args:
        df (pyspark.sql.DataFrame): The input DataFrame.
        feature_columns (list): List of feature column names.

    Returns:
        dict: A dictionary where keys are feature names and values are their VIFs.
    """

    vif_values = {}
    assembler = VectorAssembler(inputCols=feature_columns, outputCol="all_features")
    df_assembled = assembler.transform(df)

    for i, feature in enumerate(feature_columns):
        other_features = [f for f in feature_columns if f != feature]
        if not other_features:
            vif_values[feature] = 1.0  # No other features to regress against
            continue

        assembler_other = VectorAssembler(inputCols=other_features, outputCol="other_features")
        df_other_assembled = assembler_other.transform(df).select(col(feature).cast(DoubleType()), col("other_features"))

        lr = LinearRegression(featuresCol="other_features", labelCol=feature)
        model = lr.fit(df_other_assembled)
        r_squared = model.summary.r2
        if r_squared == 1.0:
            vif_values[feature] = float('inf')
        else:
            vif_values[feature] = 1.0 / (1.0 - r_squared)

    return vif_values

def remove_high_vif_spark(df, feature_columns, vif_threshold=10.0):
    """
    Removes features with a VIF above the specified threshold from a Spark DataFrame iteratively.

    Args:
        df (pyspark.sql.DataFrame): The input DataFrame.
        feature_columns (list): List of current feature column names.
        vif_threshold (float): The threshold for VIF.

    Returns:
        pyspark.sql.DataFrame: The DataFrame after removing high VIF features.
        list: List of features removed due to high VIF.
    """

    removed_features = []
    current_features = feature_columns[:]

    while True:
        if len(current_features) < 2:
            break  # Cannot calculate VIF with less than 2 features

        vif_results = calculate_vif_spark(df, current_features)
        sorted_vif = sorted(vif_results.items(), key=lambda item: item[1], reverse=True)

        highest_vif_feature, highest_vif = sorted_vif[0]

        if highest_vif > vif_threshold:
            print(f"Removing feature '{highest_vif_feature}' with VIF: {highest_vif:.2f}")
            df = df.drop(highest_vif_feature)
            removed_features.append(highest_vif_feature)
            current_features.remove(highest_vif_feature)
        else:
            break  # No more features above the threshold

    return df, current_features, removed_features

print("\nCalculating VIF...")
df_vif_removed, feature_columns_vif_removed, vif_removed_columns = remove_high_vif_spark(df_tratado, feature_columns_tratados, vif_threshold=10.0)
print(f"\nFeatures remaining after VIF removal: {feature_columns_vif_removed}")
print(f"Features removed due to high VIF: {vif_removed_columns}")

# ### 5. Evaluate Correlation and Remove

def evaluate_correlation_spark(df, feature_columns, threshold_correlacao=0.8):
    """
    Evaluates the correlation between features in a Spark DataFrame.

    Args:
        df (pyspark.sql.DataFrame): The input DataFrame.
        feature_columns (list): List of feature column names.
        threshold_correlacao (float): The absolute correlation threshold.

    Returns:
        list: A list of tuples containing pairs of highly correlated features and their correlation coefficient.
    """

    assembler = VectorAssembler(inputCols=feature_columns, outputCol="features_corr")
    df_vectorized = assembler.transform(df).select("features_corr")

    corr_matrix = Correlation.corr(df_vectorized, "features_corr").head()[0].toArray()

    highly_correlated_pairs = []
    for i in range(len(feature_columns)):
        for j in range(i + 1, len(feature_columns)):
            if abs(corr_matrix[i, j]) > threshold_correlacao:
                highly_correlated_pairs.append((feature_columns[i], feature_columns[j], corr_matrix[i, j]))

    return highly_correlated_pairs

def remove_correlation_spark(df, feature_columns, highly_correlated_pairs):
    """
    Removes highly correlated features from a Spark DataFrame.
    Keeps the first feature of each correlated pair found.

    Args:
        df (pyspark.sql.DataFrame): The input DataFrame.
        feature_columns (list): List of feature column names.
        highly_correlated_pairs (list): List of tuples of highly correlated features.

    Returns:
        pyspark.sql.DataFrame: The DataFrame after removing features.
        list: List of columns removed due to high correlation.
    """

    colunas_para_remover_corr = set()
    for col1, col2, _ in highly_correlated_pairs:
        if col2 in feature_columns and col2 not in colunas_para_remover_corr:
            colunas_para_remover_corr.add(col2)

    df = df.drop(*list(colunas_para_remover_corr))
    feature_columns_sem_corr = [col for col in feature_columns if col not in colunas_para_remover_corr]

    print(f"\nPairs of highly correlated features (> 0.8): {highly_correlated_pairs}")
    print(f"\nColumns removed due to high correlation: {list(colunas_para_remover_corr)}")

    return df, feature_columns_sem_corr, list(colunas_para_remover_corr)

print("\nEvaluating and removing highly correlated features...")
highly_correlated = evaluate_correlation_spark(df_vif_removed, feature_columns_vif_removed)
df_sem_corr, feature_columns_sem_corr, colunas_removidas_corr = remove_correlation_spark(df_vif_removed, feature_columns_vif_removed, highly_correlated)

# ### 6. Feature Selection for Minority Class

# Split data into training and testing sets
train_df, test_df = df_sem_corr.randomSplit([0.7, 0.3], seed=42)

# **Handle class imbalance using oversampling (simple example)**
major_class_count = train_df.filter(col(target_column) == 0).count() # Assuming 0 is the majority class
minor_class_count = train_df.filter(col(target_column) == 1).count() # Assuming 1 is the minority class
oversampling_ratio = major_class_count / minor_class_count

train_df_balanced = train_df.unionAll([train_df.filter(col(target_column) == 1)] * int(oversampling_ratio))
print(f"\nDistribution of the target class in the balanced training set:")
train_df_balanced.groupBy(target_column).count().withColumn("percentage", col("count") / train_df_balanced.count()).show()

# VectorAssembler to combine features into a vector
assembler_balanced = VectorAssembler(inputCols=feature_columns_sem_corr, outputCol="features")
train_df_assembled = assembler_balanced.transform(train_df_balanced).select("features", col(target_column).alias("label"))
test_df_assembled = assembler_balanced.transform(test_df).select("features", col(target_column).alias("label"))

# Feature Scaling
scaler = StandardScaler(inputCol="features", outputCol="scaledFeatures")
scaler_model = scaler.fit(train_df_assembled)
train_df_scaled = scaler_model.transform(train_df_assembled)
test_df_scaled = scaler_model.transform(test_df_assembled)

# --- Feature Selection Methods ---

# 1. Recursive Feature Elimination (RFE)
lr_rfe = LogisticRegression(featuresCol="scaledFeatures", labelCol="label", weightCol=None, predictionCol="prediction", probabilityCol="probability", rawPredictionCol="rawPrediction")
rfe = RFE(estimator=lr_rfe, numFeatures=50, featuresCol="scaledFeatures", labelCol="label")
rfe_model = rfe.fit(train_df_scaled)
train_df_selected_rfe = rfe_model.transform(train_df_scaled)
test_df_selected_rfe = rfe_model.transform(test_df_scaled)
selected_features_rfe = [feature_columns_sem_corr[i] for i in sorted(rfe_model.featureImportances.indices.tolist())]
print(f"\nFeatures selected by RFE ({len(selected_features_rfe)}): {selected_features_rfe}")

# 2. SelectFromModel (with Logistic Regression L1 Regularization)
lr_l1 = LogisticRegression(featuresCol="scaledFeatures", labelCol="label", penalty="l1", solver="liblinear", weightCol=None, predictionCol="prediction", probabilityCol="probability", rawPredictionCol="rawPrediction", elasticNetParam=0.0) # elasticNetParam=0.0 for L1
selector_l1 = SelectFromModel(estimator=lr_l1, threshold=1e-6) # Adjust threshold as needed
selector_model_l1 = selector_l1.fit(train_df_scaled)
train_df_selected_l1 = selector_model_l1.transform(train_df_scaled)
test_df_selected_l1 = selector_model_l1.transform(test_df_scaled)
selected_features_l1_indices = selector_model_l1.selectedFeatures
selected_features_l1 = [feature_columns_sem_corr[i] for i in selected_features_l1_indices]
print(f"\nFeatures selected by L1 Regularization ({len(selected_features_l1)}): {selected_features_l1}")

# ### 7. Model Training and Evaluation (with RFE selected features as an example)

# Train the logistic regression model with RFE selected features
lr_final = LogisticRegression(featuresCol="features", labelCol="label", weightCol=None, predictionCol="prediction", probabilityCol="probability", rawPredictionCol="rawPrediction")
pipeline_rfe = Pipeline(stages=[assembler_balanced, scaler, rfe, lr_final])
model_rfe = pipeline_rfe.fit(train_df_balanced)
predictions_rfe = model_rfe.transform(test_df)

# Evaluate the model
evaluator_roc = BinaryClassificationEvaluator(rawPredictionCol="rawPrediction", labelCol="label", metricName="areaUnderROC")
auc_rfe = evaluator_roc.evaluate(predictions_rfe)
print(f"\nAUC-ROC no conjunto de teste (com features selecionadas por RFE): {auc_rfe:.4f}")

# Métricas de classificação (Precision, Recall, F1-Score)
evaluator_precision = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="label", metricName="weightedPrecision")
precision_rfe = evaluator_precision.evaluate(predictions_rfe)
print(f"Precisão Ponderada (Weighted Precision): {precision_rfe:.4f}")

evaluator_recall = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="label", metricName="weightedRecall")
recall_rfe = evaluator_recall.evaluate(predictions_rfe)
print(f"Recall Ponderado (Weighted Recall): {recall_rfe:.4f}")

evaluator_f1 = MulticlassClassificationEvaluator(predictionCol="prediction", labelCol="label", metricName="f1")
f1_rfe = evaluator_f1.evaluate(predictions_rfe)
print(f"F1-Score Ponderado (Weighted F1-Score): {f1_rfe:.4f}")

# Coletar previsões e rótulos para a matriz de confusão
y_true = predictions_rfe.select("label").toPandas()
y_pred = predictions_rfe.select("prediction").toPandas()

# Gerar e exibir a matriz de confusão
cm = confusion_matrix(y_true, y_pred)
labels = sorted(y_true['label'].unique()) # Obter os rótulos únicos
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=labels, yticklabels=labels)
plt.xlabel('Rótulo Previsto')
plt.ylabel('Rótulo Verdadeiro')
plt.title('Matriz de Confusão (com features selecionadas por RFE)')
plt.show()

# Relatório de Classificação Detalhado
print("\nRelatório de Classificação Detalhado:")
print(classification_report(y_true, y_pred, target_names=[str(label) for label in labels]))

# ### 8. Finalizar Spark Session

spark.stop()
