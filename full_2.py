# -*- coding: utf-8 -*-
"""Notebook de Pipeline de Seleção de Features para Regressão Logística com Classe Minoritária.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1-YOUR_GOOGLE_DRIVE_FILE_ID
"""

# ## Pipeline de Seleção de Features para Regressão Logística com Classe Minoritária

# Este notebook demonstra um pipeline completo para realizar seleção de features em um conjunto de dados com muitas features (900) e uma classe minoritária (4%), visando treinar um modelo de regressão logística.

# ### 1. Importar Bibliotecas

import pandas as pd
import numpy as np
from statsmodels.stats.outliers_influence import variance_inflation_factor
from statsmodels.tools.tools import add_constant
from sklearn.feature_selection import SelectKBest, mutual_info_classif, RFECV, SelectFromModel
from sklearn.model_selection import StratifiedKFold
from sklearn.linear_model import LogisticRegression
from sklearn.preprocessing import MinMaxScaler, StandardScaler
from sklearn.model_selection import train_test_split
from sklearn.metrics import classification_report, roc_auc_score, confusion_matrix
import matplotlib.pyplot as plt
import seaborn as sns
from imblearn.over_sampling import SMOTE

# ### 2. Carregar e Explorar os Dados

# **Substitua 'seu_arquivo.csv' pelo caminho do seu arquivo de dados.**
try:
    df = pd.read_csv('seu_arquivo.csv')
    print("Dados carregados com sucesso!")
    print(f"Número de amostras: {len(df)}")
    print(f"Número de features: {df.shape[1] - 1} (assumindo que a última coluna é a target)")
    print("\nInformações básicas do DataFrame:")
    df.info()
    print("\nDistribuição da classe target:")
    print(df.iloc[:, -1].value_counts(normalize=True) * 100) # Assumindo que a última coluna é a target
except FileNotFoundError:
    print("Erro: Arquivo 'seu_arquivo.csv' não encontrado. Por favor, verifique o caminho do arquivo.")
    exit()

# Separar features (X) e variável alvo (y)
X = df.iloc[:, :-1]
y = df.iloc[:, -1]

# ### 3. Avaliação e Tratamento de Missing Values

def avaliar_e_tratar_missing(df):
    """
    Avalia e trata missing values em um DataFrame.

    Args:
        df (pd.DataFrame): O DataFrame de entrada.

    Returns:
        pd.DataFrame: O DataFrame após o tratamento de missing values.
    """

    # 1. Identificar a porcentagem de missing values por coluna
    missing_percentage = (df.isnull().sum() / len(df)) * 100
    print("Porcentagem de Missing Values por Coluna:")
    print(missing_percentage[missing_percentage > 0].sort_values(ascending=False))  # Exibe apenas colunas com missing values

    # 2. Decidir como tratar (exemplo: remover colunas com muitos missing, imputar outros)
    # Exemplo: Remover colunas com mais de 50% de missing
    threshold_remover_colunas = 50
    colunas_para_remover = missing_percentage[missing_percentage > threshold_remover_colunas].index
    df = df.drop(columns=colunas_para_remover)
    print(f"\nColunas removidas ({len(colunas_para_remover)}): {list(colunas_para_remover)}")

    # Exemplo: Imputar missing values restantes com a mediana (robusto a outliers)
    colunas_para_imputar = df.columns[df.isnull().any()].tolist()
    for coluna in colunas_para_imputar:
        df[coluna] = df[coluna].fillna(df[coluna].median())
    print(f"\nMissing values imputados com mediana nas colunas: {colunas_para_imputar}")

    # 3. Verificar se ainda há missing values
    print("\nMissing values restantes:")
    print(df.isnull().sum().sum())

    return df

X_tratado = avaliar_e_tratar_missing(X.copy())

# ### 4. Avaliação de Multicolinearidade (VIF) e Remoção

def calcular_vif(df):
    """
    Calcula o Variance Inflation Factor (VIF) para cada feature em um DataFrame.

    Args:
        df (pd.DataFrame): O DataFrame de entrada.

    Returns:
        pd.DataFrame: Um DataFrame com os VIFs.
    """

    # Adicionar uma constante para o cálculo do VIF
    df_vif = add_constant(df)
    vif_data = pd.DataFrame()
    vif_data["feature"] = df_vif.columns
    vif_data["VIF"] = [variance_inflation_factor(df_vif.values, i) for i in range(df_vif.shape[1])]

    return vif_data.sort_values(by="VIF", ascending=False)

def remover_alta_multicolinearidade(df, threshold_vif=10):
    """
    Remove features com alta multicolinearidade (VIF acima de um threshold).

    Args:
        df (pd.DataFrame): O DataFrame de entrada.
        threshold_vif (float): O threshold para o VIF.

    Returns:
        pd.DataFrame: O DataFrame após a remoção de features.
    """

    df_sem_multicolinearidade = df.copy()
    print("\nIniciando remoção de multicolinearidade (VIF > {}):".format(threshold_vif))
    while True:
        vif_resultado = calcular_vif(add_constant(df_sem_multicolinearidade))
        maiores_vif = vif_resultado[vif_resultado['VIF'] > threshold_vif]
        if maiores_vif.shape[0] <= 1: # Parar se apenas a constante ou nenhuma feature acima do threshold
            break
        feature_para_remover = maiores_vif.iloc[0, 0]
        if feature_para_remover == 'const':
            feature_para_remover = maiores_vif.iloc[1, 0] if maiores_vif.shape[0] > 1 else None
            if feature_para_remover is None:
                break
        df_sem_multicolinearidade = df_sem_multicolinearidade.drop(columns=[feature_para_remover])
        print(f"Removendo feature: {feature_para_remover}, VIF: {maiores_vif[maiores_vif['feature'] == feature_para_remover]['VIF'].values[0]:.2f}")
    print("\nRemoção de multicolinearidade concluída.")
    return df_sem_multicolinearidade

X_sem_vif = remover_alta_multicolinearidade(X_tratado.copy(), threshold_vif=10)

# ### 5. Avaliação de Correlação e Remoção

def avaliar_correlacao(df, threshold_correlacao=0.8):
    """
    Avalia a correlação entre as features em um DataFrame.

    Args:
        df (pd.DataFrame): O DataFrame de entrada.
        threshold_correlacao (float): O threshold para a correlação.

    Returns:
        list: Uma lista de pares de features altamente correlacionadas.
    """

    correlacao_matrix = df.corr().abs()
    upper_tri = correlacao_matrix.where(np.triu(np.ones(correlacao_matrix.shape),k=1).astype(bool))
    colunas_para_remover = [coluna for coluna in upper_tri.columns if any(upper_tri[coluna] > threshold_correlacao)]
    pares_correlacionados = []
    for coluna in colunas_para_remover:
        colunas_correlacionadas = upper_tri.index[upper_tri[coluna] > threshold_correlacao].tolist()
        for outra_coluna in colunas_correlacionadas:
            pares_correlacionados.append((coluna, outra_coluna, correlacao_matrix[coluna][outra_coluna]))

    return pares_correlacionados

def remover_correlacao(df, threshold_correlacao=0.8):
    """
    Remove features altamente correlacionadas.

    Args:
        df (pd.DataFrame): O DataFrame de entrada.
        threshold_correlacao (float): O threshold para a correlação.

    Returns:
        pd.DataFrame: O DataFrame após a remoção de features.
    """
    df_sem_correlacao = df.copy()
    correlacao_matrix = df_sem_correlacao.corr().abs()
    upper_tri = correlacao_matrix.where(np.triu(np.ones(correlacao_matrix.shape),k=1).astype(bool))
    colunas_para_remover = [coluna for coluna in upper_tri.columns if any(upper_tri[coluna] > threshold_correlacao)]
    df_sem_correlacao = df_sem_correlacao.drop(columns=colunas_para_remover)
    print(f"\nColunas removidas por alta correlação (> {threshold_correlacao}): {colunas_para_remover}")
    return df_sem_correlacao

X_sem_correlacao = remover_correlacao(X_sem_vif.copy(), threshold_correlacao=0.8)

# ### 6. Seleção de Features para Classe Minoritária

# Dividir os dados em treino e teste antes da seleção de features para evitar vazamento de dados.
X_train, X_test, y_train, y_test = train_test_split(X_sem_correlacao, y, test_size=0.3, stratify=y, random_state=42)

# **Oversampling da classe minoritária usando SMOTE (apenas no conjunto de treino)**
smote = SMOTE(random_state=42)
X_train_resampled, y_train_resampled = smote.fit_resample(X_train, y_train)

print(f"\nDistribuição da classe target original (treino):\n{y_train.value_counts(normalize=True) * 100}")
print(f"\nDistribuição da classe target após SMOTE (treino):\n{y_train_resampled.value_counts(normalize=True) * 100}")

def selecionar_features_mi(X, y, k=50):
    """
    Seleciona as k melhores features usando Informação Mútua.

    Args:
        X (pd.DataFrame): As features.
        y (pd.Series): A variável alvo.
        k (int): Número de features a selecionar.

    Returns:
        pd.DataFrame: As features selecionadas.
    """

    selector = SelectKBest(mutual_info_classif, k=k)
    selector.fit(X, y)
    colunas_selecionadas = X.columns[selector.get_support()]
    return X[colunas_selecionadas], colunas_selecionadas

def selecionar_features_rfecv(X, y, cv=5, step=1):
    """
    Seleciona as features usando Recursive Feature Elimination com Cross-Validation (RFECV).

    Args:
        X (pd.DataFrame): As features.
        y (pd.Series): A variável alvo.
        cv (int): Número de folds na validação cruzada.
        step (int): Número de features a remover em cada iteração.

    Returns:
        pd.DataFrame: As features selecionadas.
    """

    escalador = StandardScaler() # Escalar os dados é importante para RFECV com regressão logística
    X_escalado = escalador.fit_transform(X)

    modelo_base = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42) # class_weight para lidar com desbalanceamento
    rfecv = RFECV(estimator=modelo_base, step=step, cv=StratifiedKFold(n_splits=cv, shuffle=True, random_state=42), scoring='roc_auc') # Use ROC AUC para classe desbalanceada
    rfecv.fit(X_escalado, y)

    print(f"Número ideal de features (RFECV): {rfecv.n_features_}")
    colunas_selecionadas = X.columns[rfecv.support_]
    return X[colunas_selecionadas], colunas_selecionadas

def selecionar_features_l1(X, y, alpha=0.1):
    """
    Seleciona features usando SelectFromModel com Regressão Logística L1 (Lasso).

    Args:
        X (pd.DataFrame): As features.
        y (pd.Series): A variável alvo.
        alpha (float): Parâmetro de regularização L1.

    Returns:
        pd.DataFrame: As features selecionadas.
    """
    escalador = StandardScaler()
    X_escalado = escalador.fit_transform(X)
    modelo_l1 = LogisticRegression(penalty='l1', solver='liblinear', C=1/alpha, class_weight='balanced', random_state=42)
    modelo_l1.fit(X_escalado, y)
    selector = SelectFromModel(modelo_l1, prefit=True)
    colunas_selecionadas = X.columns[selector.get_support()]
    return X[colunas_selecionadas], colunas_selecionadas

# Aplicar seleção de features no conjunto de treino resampled
X_train_mi, cols_mi = selecionar_features_mi(X_train_resampled, y_train_resampled, k=50)
print(f"\nFeatures selecionadas por Informação Mútua ({len(cols_mi)}): {list(cols_mi)}")

X_train_rfecv, cols_rfecv = selecionar_features_rfecv(X_train_resampled, y_train_resampled, cv=5, step=5)
print(f"\nFeatures selecionadas por RFECV ({len(cols_rfecv)}): {list(cols_rfecv)}")

X_train_l1, cols_l1 = selecionar_features_l1(X_train_resampled, y_train_resampled, alpha=0.1)
print(f"\nFeatures selecionadas por L1 Regularization ({len(cols_l1)}): {list(cols_l1)}")

# ### 7. Treinamento e Avaliação do Modelo (com features selecionadas por RFECV como exemplo)

# Preparar dados de treino e teste com as features selecionadas por RFECV
X_train_selected = X_train_resampled[cols_rfecv]
X_test_selected = X_test[cols_rfecv]

# Escalonar os dados de treino e teste
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train_selected)
X_test_scaled = scaler.transform(X_test_selected)

# Treinar o modelo de regressão logística
modelo_rl = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)
modelo_rl.fit(X_train_scaled, y_train_resampled)

# Fazer previsões no conjunto de teste
y_pred = modelo_rl.predict(X_test_scaled)
y_prob = modelo_rl.predict_proba(X_test_scaled)[:, 1]

# Avaliar o modelo
print("\nRelatório de Classificação (com features selecionadas por RFECV):")
print(classification_report(y_test, y_pred))
print(f"\nAUC-ROC (com features selecionadas por RFECV): {roc_auc_score(y_test, y_prob):.4f}")

# Matriz de Confusão
cm = confusion_matrix(y_test, y_pred)
plt.figure(figsize=(8, 6))
sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')
plt.xlabel('Predicted Label')
plt.ylabel('True Label')
plt.title('Matriz de Confusão (com features selecionadas por RFECV)')
plt.show()

# ### 8. Comparação de Diferentes Seleções de Features (Opcional)

# Você pode repetir o passo 7 para as features selecionadas por Informação Mútua e L1 para comparar o desempenho dos modelos com diferentes conjuntos de features.

# Exemplo com features selecionadas por Informação Mútua:
X_train_mi_selected = X_train_resampled[cols_mi]
X_test_mi_selected = X_test[cols_mi]
X_train_mi_scaled = scaler.fit_transform(X_train_mi_selected)
X_test_mi_scaled = scaler.transform(X_test_mi_selected)
modelo_rl_mi = LogisticRegression(solver='liblinear', class_weight='balanced', random_state=42)
modelo_rl_mi.fit(X_train_mi_scaled, y_train_resampled)
y_pred_mi = modelo_rl_mi.
